16:04:36,593 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,593 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,593 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,593 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,593 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,601 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,601 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,603 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,603 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,604 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,605 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,605 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,605 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,605 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,605 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,605 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,607 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,607 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,607 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,607 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,608 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,608 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,608 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,608 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:75 
16:04:36,608 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 52 (collect at JobClusterUtils.java:75) with 2 output partitions 
16:04:36,608 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 55 (collect at JobClusterUtils.java:75) 
16:04:36,608 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,608 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,608 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 55) 
16:04:36,608 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,608 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 55 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,608 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 55) 
16:04:36,609 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_61 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,609 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_61 locally took  0 ms 
16:04:36,609 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_61 without replication took  0 ms 
16:04:36,610 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_61_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,610 [dispatcher-event-loop-1] INFO  spark.storage.BlockManagerInfo  - Added broadcast_61_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,610 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_61_piece0 
16:04:36,610 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_61_piece0 
16:04:36,610 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_61_piece0 locally took  0 ms 
16:04:36,610 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_61_piece0 without replication took  0 ms 
16:04:36,610 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 61 from broadcast at DAGScheduler.scala:1039 
16:04:36,611 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 55 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,611 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 55.0 with 2 tasks 
16:04:36,611 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 55.0: 3 
16:04:36,611 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 55.0: NO_PREF, ANY 
16:04:36,611 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_55.0, runningTasks: 0 
16:04:36,611 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 55.0 (TID 110, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,611 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 55.0 (TID 111, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
16:04:36,611 [Executor task launch worker for task 111] INFO  spark.executor.Executor         - Running task 1.0 in stage 55.0 (TID 111) 
16:04:36,611 [Executor task launch worker for task 110] INFO  spark.executor.Executor         - Running task 0.0 in stage 55.0 (TID 110) 
16:04:36,612 [Executor task launch worker for task 111] DEBUG spark.storage.BlockManager      - Getting local block broadcast_61 
16:04:36,612 [Executor task launch worker for task 111] DEBUG spark.storage.BlockManager      - Level for block broadcast_61 is StorageLevel(disk, memory, deserialized, 1 replicas) 
16:04:36,612 [Executor task launch worker for task 110] INFO  spark.executor.Executor         - Finished task 0.0 in stage 55.0 (TID 110). 614 bytes result sent to driver 
16:04:36,612 [Executor task launch worker for task 111] INFO  spark.executor.Executor         - Finished task 1.0 in stage 55.0 (TID 111). 614 bytes result sent to driver 
16:04:36,612 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_55.0, runningTasks: 1 
16:04:36,612 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSetManager  - No tasks for locality level NO_PREF, so moving to locality level ANY 
16:04:36,612 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_55.0, runningTasks: 0 
16:04:36,612 [task-result-getter-2] INFO  spark.scheduler.TaskSetManager  - Finished task 0.0 in stage 55.0 (TID 110) in 1 ms on localhost (executor driver) (1/2) 
16:04:36,612 [task-result-getter-3] INFO  spark.scheduler.TaskSetManager  - Finished task 1.0 in stage 55.0 (TID 111) in 1 ms on localhost (executor driver) (2/2) 
16:04:36,612 [task-result-getter-3] INFO  spark.scheduler.TaskSchedulerImpl  - Removed TaskSet 55.0, whose tasks have all completed, from pool  
16:04:36,612 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - ResultStage 55 (collect at JobClusterUtils.java:75) finished in 0.004 s 
16:04:36,612 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - After removal of stage 55, remaining stages = 0 
16:04:36,612 [main] INFO  spark.scheduler.DAGScheduler    - Job 52 finished: collect at JobClusterUtils.java:75, took 0.004573 s 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++ 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object) 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator) 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 2 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 2 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      <function0> 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,613 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,614 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,614 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,614 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,614 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,614 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,614 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,614 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,615 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,616 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,617 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,617 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,617 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,617 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,617 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,617 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,617 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,617 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:73 
16:04:36,618 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 53 (collect at JobClusterUtils.java:73) with 2 output partitions 
16:04:36,618 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 56 (collect at JobClusterUtils.java:73) 
16:04:36,618 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,618 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,618 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 56) 
16:04:36,618 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,618 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 56 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,618 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 56) 
16:04:36,618 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_62 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,618 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_62 locally took  0 ms 
16:04:36,619 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_62 without replication took  1 ms 
16:04:36,619 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_62_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,619 [dispatcher-event-loop-0] INFO  spark.storage.BlockManagerInfo  - Added broadcast_62_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,619 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_62_piece0 
16:04:36,619 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_62_piece0 
16:04:36,619 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_62_piece0 locally took  0 ms 
16:04:36,620 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_62_piece0 without replication took  1 ms 
16:04:36,620 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 62 from broadcast at DAGScheduler.scala:1039 
16:04:36,620 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 56 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,620 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 56.0 with 2 tasks 
16:04:36,620 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 56.0: 3 
16:04:36,620 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 56.0: NO_PREF, ANY 
16:04:36,620 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_56.0, runningTasks: 0 
16:04:36,620 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 56.0 (TID 112, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,620 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 56.0 (TID 113, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
16:04:36,620 [Executor task launch worker for task 112] INFO  spark.executor.Executor         - Running task 0.0 in stage 56.0 (TID 112) 
16:04:36,620 [Executor task launch worker for task 113] INFO  spark.executor.Executor         - Running task 1.0 in stage 56.0 (TID 113) 
16:04:36,621 [Executor task launch worker for task 113] DEBUG spark.storage.BlockManager      - Getting local block broadcast_62 
16:04:36,621 [Executor task launch worker for task 113] DEBUG spark.storage.BlockManager      - Level for block broadcast_62 is StorageLevel(disk, memory, deserialized, 1 replicas) 
16:04:36,621 [Executor task launch worker for task 112] INFO  spark.executor.Executor         - Finished task 0.0 in stage 56.0 (TID 112). 614 bytes result sent to driver 
16:04:36,621 [Executor task launch worker for task 113] INFO  spark.executor.Executor         - Finished task 1.0 in stage 56.0 (TID 113). 614 bytes result sent to driver 
16:04:36,621 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_56.0, runningTasks: 1 
16:04:36,621 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSetManager  - No tasks for locality level NO_PREF, so moving to locality level ANY 
16:04:36,621 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_56.0, runningTasks: 0 
16:04:36,622 [task-result-getter-1] INFO  spark.scheduler.TaskSetManager  - Finished task 1.0 in stage 56.0 (TID 113) in 2 ms on localhost (executor driver) (1/2) 
16:04:36,622 [task-result-getter-0] INFO  spark.scheduler.TaskSetManager  - Finished task 0.0 in stage 56.0 (TID 112) in 2 ms on localhost (executor driver) (2/2) 
16:04:36,622 [task-result-getter-0] INFO  spark.scheduler.TaskSchedulerImpl  - Removed TaskSet 56.0, whose tasks have all completed, from pool  
16:04:36,622 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - ResultStage 56 (collect at JobClusterUtils.java:73) finished in 0.004 s 
16:04:36,622 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - After removal of stage 56, remaining stages = 0 
16:04:36,622 [main] INFO  spark.scheduler.DAGScheduler    - Job 53 finished: collect at JobClusterUtils.java:73, took 0.004391 s 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++ 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object) 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator) 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 2 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 2 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      <function0> 
16:04:36,622 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,623 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,624 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,625 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,626 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,626 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,626 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,626 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:75 
16:04:36,626 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 54 (collect at JobClusterUtils.java:75) with 2 output partitions 
16:04:36,626 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 57 (collect at JobClusterUtils.java:75) 
16:04:36,626 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,626 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,626 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 57) 
16:04:36,626 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,626 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 57 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,626 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 57) 
16:04:36,626 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_63 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,627 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_63 locally took  1 ms 
16:04:36,627 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_63 without replication took  1 ms 
16:04:36,627 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_63_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,628 [dispatcher-event-loop-1] INFO  spark.storage.BlockManagerInfo  - Added broadcast_63_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,628 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_63_piece0 
16:04:36,628 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_63_piece0 
16:04:36,628 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_63_piece0 locally took  1 ms 
16:04:36,628 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_63_piece0 without replication took  1 ms 
16:04:36,628 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 63 from broadcast at DAGScheduler.scala:1039 
16:04:36,628 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 57 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,628 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 57.0 with 2 tasks 
16:04:36,628 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 57.0: 3 
16:04:36,628 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 57.0: NO_PREF, ANY 
16:04:36,628 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_57.0, runningTasks: 0 
16:04:36,628 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 57.0 (TID 114, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,628 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 57.0 (TID 115, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
16:04:36,629 [Executor task launch worker for task 114] INFO  spark.executor.Executor         - Running task 0.0 in stage 57.0 (TID 114) 
16:04:36,629 [Executor task launch worker for task 115] INFO  spark.executor.Executor         - Running task 1.0 in stage 57.0 (TID 115) 
16:04:36,629 [Executor task launch worker for task 114] DEBUG spark.storage.BlockManager      - Getting local block broadcast_63 
16:04:36,629 [Executor task launch worker for task 114] DEBUG spark.storage.BlockManager      - Level for block broadcast_63 is StorageLevel(disk, memory, deserialized, 1 replicas) 
16:04:36,629 [Executor task launch worker for task 114] INFO  spark.executor.Executor         - Finished task 0.0 in stage 57.0 (TID 114). 571 bytes result sent to driver 
16:04:36,629 [Executor task launch worker for task 115] INFO  spark.executor.Executor         - Finished task 1.0 in stage 57.0 (TID 115). 571 bytes result sent to driver 
16:04:36,629 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_57.0, runningTasks: 1 
16:04:36,629 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSetManager  - No tasks for locality level NO_PREF, so moving to locality level ANY 
16:04:36,630 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_57.0, runningTasks: 0 
16:04:36,630 [task-result-getter-2] INFO  spark.scheduler.TaskSetManager  - Finished task 0.0 in stage 57.0 (TID 114) in 2 ms on localhost (executor driver) (1/2) 
16:04:36,630 [task-result-getter-3] INFO  spark.scheduler.TaskSetManager  - Finished task 1.0 in stage 57.0 (TID 115) in 2 ms on localhost (executor driver) (2/2) 
16:04:36,630 [task-result-getter-3] INFO  spark.scheduler.TaskSchedulerImpl  - Removed TaskSet 57.0, whose tasks have all completed, from pool  
16:04:36,630 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - ResultStage 57 (collect at JobClusterUtils.java:75) finished in 0.004 s 
16:04:36,630 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - After removal of stage 57, remaining stages = 0 
16:04:36,630 [main] INFO  spark.scheduler.DAGScheduler    - Job 54 finished: collect at JobClusterUtils.java:75, took 0.004123 s 
16:04:36,630 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++ 
16:04:36,630 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,630 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID 
16:04:36,630 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer 
16:04:36,630 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object) 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator) 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 2 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 2 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      <function0> 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,631 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,632 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,633 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,634 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,634 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,634 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:73 
16:04:36,634 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 55 (collect at JobClusterUtils.java:73) with 2 output partitions 
16:04:36,634 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 58 (collect at JobClusterUtils.java:73) 
16:04:36,634 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,634 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,634 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 58) 
16:04:36,634 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,634 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 58 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,634 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 58) 
16:04:36,634 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_64 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,634 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_64 locally took  0 ms 
16:04:36,634 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_64 without replication took  0 ms 
16:04:36,635 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_64_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,636 [dispatcher-event-loop-0] INFO  spark.storage.BlockManagerInfo  - Added broadcast_64_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,636 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_64_piece0 
16:04:36,636 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_64_piece0 
16:04:36,636 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_64_piece0 locally took  1 ms 
16:04:36,636 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_64_piece0 without replication took  1 ms 
16:04:36,636 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 64 from broadcast at DAGScheduler.scala:1039 
16:04:36,636 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 58 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,636 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 58.0 with 2 tasks 
16:04:36,636 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 58.0: 3 
16:04:36,636 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 58.0: NO_PREF, ANY 
16:04:36,636 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_58.0, runningTasks: 0 
16:04:36,636 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 58.0 (TID 116, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,636 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 58.0 (TID 117, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
16:04:36,636 [Executor task launch worker for task 117] INFO  spark.executor.Executor         - Running task 1.0 in stage 58.0 (TID 117) 
16:04:36,636 [Executor task launch worker for task 116] INFO  spark.executor.Executor         - Running task 0.0 in stage 58.0 (TID 116) 
16:04:36,636 [Executor task launch worker for task 117] DEBUG spark.storage.BlockManager      - Getting local block broadcast_64 
16:04:36,636 [Executor task launch worker for task 117] DEBUG spark.storage.BlockManager      - Level for block broadcast_64 is StorageLevel(disk, memory, deserialized, 1 replicas) 
16:04:36,637 [Executor task launch worker for task 117] INFO  spark.executor.Executor         - Finished task 1.0 in stage 58.0 (TID 117). 571 bytes result sent to driver 
16:04:36,637 [Executor task launch worker for task 116] INFO  spark.executor.Executor         - Finished task 0.0 in stage 58.0 (TID 116). 571 bytes result sent to driver 
16:04:36,637 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_58.0, runningTasks: 1 
16:04:36,637 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSetManager  - No tasks for locality level NO_PREF, so moving to locality level ANY 
16:04:36,637 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_58.0, runningTasks: 0 
16:04:36,637 [task-result-getter-0] INFO  spark.scheduler.TaskSetManager  - Finished task 0.0 in stage 58.0 (TID 116) in 1 ms on localhost (executor driver) (1/2) 
16:04:36,637 [task-result-getter-1] INFO  spark.scheduler.TaskSetManager  - Finished task 1.0 in stage 58.0 (TID 117) in 1 ms on localhost (executor driver) (2/2) 
16:04:36,637 [task-result-getter-1] INFO  spark.scheduler.TaskSchedulerImpl  - Removed TaskSet 58.0, whose tasks have all completed, from pool  
16:04:36,637 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - ResultStage 58 (collect at JobClusterUtils.java:73) finished in 0.003 s 
16:04:36,637 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - After removal of stage 58, remaining stages = 0 
16:04:36,637 [main] INFO  spark.scheduler.DAGScheduler    - Job 55 finished: collect at JobClusterUtils.java:73, took 0.004435 s 
16:04:36,637 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++ 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object) 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator) 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 2 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 2 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      <function0> 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,638 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,639 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,640 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,641 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,641 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,641 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,641 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,641 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:75 
16:04:36,641 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 56 (collect at JobClusterUtils.java:75) with 2 output partitions 
16:04:36,641 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 59 (collect at JobClusterUtils.java:75) 
16:04:36,641 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,641 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,641 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 59) 
16:04:36,641 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,641 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 59 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,641 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 59) 
16:04:36,642 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_65 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,642 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_65 locally took  0 ms 
16:04:36,642 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_65 without replication took  0 ms 
16:04:36,643 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_65_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,643 [dispatcher-event-loop-1] INFO  spark.storage.BlockManagerInfo  - Added broadcast_65_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,643 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_65_piece0 
16:04:36,643 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_65_piece0 
16:04:36,643 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_65_piece0 locally took  0 ms 
16:04:36,643 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_65_piece0 without replication took  0 ms 
16:04:36,643 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 65 from broadcast at DAGScheduler.scala:1039 
16:04:36,643 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 59 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,643 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 59.0 with 2 tasks 
16:04:36,643 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 59.0: 3 
16:04:36,643 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 59.0: NO_PREF, ANY 
16:04:36,643 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_59.0, runningTasks: 0 
16:04:36,644 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 59.0 (TID 118, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,644 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 59.0 (TID 119, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
16:04:36,644 [Executor task launch worker for task 119] INFO  spark.executor.Executor         - Running task 1.0 in stage 59.0 (TID 119) 
16:04:36,644 [Executor task launch worker for task 118] INFO  spark.executor.Executor         - Running task 0.0 in stage 59.0 (TID 118) 
16:04:36,644 [Executor task launch worker for task 118] DEBUG spark.storage.BlockManager      - Getting local block broadcast_65 
16:04:36,644 [Executor task launch worker for task 118] DEBUG spark.storage.BlockManager      - Level for block broadcast_65 is StorageLevel(disk, memory, deserialized, 1 replicas) 
16:04:36,644 [Executor task launch worker for task 119] INFO  spark.executor.Executor         - Finished task 1.0 in stage 59.0 (TID 119). 571 bytes result sent to driver 
16:04:36,644 [Executor task launch worker for task 118] INFO  spark.executor.Executor         - Finished task 0.0 in stage 59.0 (TID 118). 571 bytes result sent to driver 
16:04:36,644 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_59.0, runningTasks: 1 
16:04:36,644 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSetManager  - No tasks for locality level NO_PREF, so moving to locality level ANY 
16:04:36,645 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_59.0, runningTasks: 0 
16:04:36,645 [task-result-getter-2] INFO  spark.scheduler.TaskSetManager  - Finished task 1.0 in stage 59.0 (TID 119) in 1 ms on localhost (executor driver) (1/2) 
16:04:36,645 [task-result-getter-3] INFO  spark.scheduler.TaskSetManager  - Finished task 0.0 in stage 59.0 (TID 118) in 2 ms on localhost (executor driver) (2/2) 
16:04:36,645 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - ResultStage 59 (collect at JobClusterUtils.java:75) finished in 0.003 s 
16:04:36,645 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - After removal of stage 59, remaining stages = 0 
16:04:36,645 [task-result-getter-3] INFO  spark.scheduler.TaskSchedulerImpl  - Removed TaskSet 59.0, whose tasks have all completed, from pool  
16:04:36,645 [main] INFO  spark.scheduler.DAGScheduler    - Job 56 finished: collect at JobClusterUtils.java:75, took 0.003929 s 
16:04:36,645 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++ 
16:04:36,645 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,645 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID 
16:04:36,645 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer 
16:04:36,645 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,645 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object) 
16:04:36,645 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator) 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 2 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 2 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      <function0> 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,646 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,647 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,648 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,649 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,649 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:73 
16:04:36,649 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 57 (collect at JobClusterUtils.java:73) with 2 output partitions 
16:04:36,649 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 60 (collect at JobClusterUtils.java:73) 
16:04:36,649 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,649 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,649 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 60) 
16:04:36,649 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,649 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 60 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,649 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 60) 
16:04:36,650 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_66 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,650 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_66 locally took  1 ms 
16:04:36,650 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_66 without replication took  1 ms 
16:04:36,651 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_66_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,651 [dispatcher-event-loop-0] INFO  spark.storage.BlockManagerInfo  - Added broadcast_66_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,651 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_66_piece0 
16:04:36,651 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_66_piece0 
16:04:36,651 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_66_piece0 locally took  0 ms 
16:04:36,651 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_66_piece0 without replication took  0 ms 
16:04:36,651 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 66 from broadcast at DAGScheduler.scala:1039 
16:04:36,651 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 60 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,651 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 60.0 with 2 tasks 
16:04:36,652 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 60.0: 3 
16:04:36,652 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 60.0: NO_PREF, ANY 
16:04:36,652 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_60.0, runningTasks: 0 
16:04:36,652 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 60.0 (TID 120, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,653 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 60.0 (TID 121, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
16:04:36,653 [Executor task launch worker for task 121] INFO  spark.executor.Executor         - Running task 1.0 in stage 60.0 (TID 121) 
16:04:36,653 [Executor task launch worker for task 120] INFO  spark.executor.Executor         - Running task 0.0 in stage 60.0 (TID 120) 
16:04:36,654 [Executor task launch worker for task 120] DEBUG spark.storage.BlockManager      - Getting local block broadcast_66 
16:04:36,654 [Executor task launch worker for task 120] DEBUG spark.storage.BlockManager      - Level for block broadcast_66 is StorageLevel(disk, memory, deserialized, 1 replicas) 
16:04:36,655 [Executor task launch worker for task 121] INFO  spark.executor.Executor         - Finished task 1.0 in stage 60.0 (TID 121). 614 bytes result sent to driver 
16:04:36,655 [Executor task launch worker for task 120] INFO  spark.executor.Executor         - Finished task 0.0 in stage 60.0 (TID 120). 614 bytes result sent to driver 
16:04:36,655 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_60.0, runningTasks: 1 
16:04:36,655 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSetManager  - No tasks for locality level NO_PREF, so moving to locality level ANY 
16:04:36,655 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_60.0, runningTasks: 0 
16:04:36,655 [task-result-getter-0] INFO  spark.scheduler.TaskSetManager  - Finished task 1.0 in stage 60.0 (TID 121) in 3 ms on localhost (executor driver) (1/2) 
16:04:36,655 [task-result-getter-1] INFO  spark.scheduler.TaskSetManager  - Finished task 0.0 in stage 60.0 (TID 120) in 3 ms on localhost (executor driver) (2/2) 
16:04:36,655 [task-result-getter-1] INFO  spark.scheduler.TaskSchedulerImpl  - Removed TaskSet 60.0, whose tasks have all completed, from pool  
16:04:36,655 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - ResultStage 60 (collect at JobClusterUtils.java:73) finished in 0.006 s 
16:04:36,655 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - After removal of stage 60, remaining stages = 0 
16:04:36,655 [main] INFO  spark.scheduler.DAGScheduler    - Job 57 finished: collect at JobClusterUtils.java:73, took 0.006474 s 
16:04:36,655 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++ 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object) 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator) 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 2 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 2 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      <function0> 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,656 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,657 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,657 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,657 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,657 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,657 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,657 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,657 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,658 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,659 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,659 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,659 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,659 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,659 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,659 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,659 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,660 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,660 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,660 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,660 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,660 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,660 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,660 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,660 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:75 
16:04:36,660 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 58 (collect at JobClusterUtils.java:75) with 2 output partitions 
16:04:36,660 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 61 (collect at JobClusterUtils.java:75) 
16:04:36,660 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,660 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,660 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 61) 
16:04:36,660 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,660 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 61 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,660 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 61) 
16:04:36,661 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_67 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,661 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_67 locally took  0 ms 
16:04:36,661 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_67 without replication took  0 ms 
16:04:36,661 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_67_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,662 [dispatcher-event-loop-1] INFO  spark.storage.BlockManagerInfo  - Added broadcast_67_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,662 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_67_piece0 
16:04:36,662 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_67_piece0 
16:04:36,662 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_67_piece0 locally took  1 ms 
16:04:36,662 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_67_piece0 without replication took  1 ms 
16:04:36,662 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 67 from broadcast at DAGScheduler.scala:1039 
16:04:36,662 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 61 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,662 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 61.0 with 2 tasks 
16:04:36,662 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 61.0: 3 
16:04:36,662 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 61.0: NO_PREF, ANY 
16:04:36,662 [dispatcher-event-loop-0] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_61.0, runningTasks: 0 
16:04:36,662 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 61.0 (TID 122, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,662 [dispatcher-event-loop-0] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 61.0 (TID 123, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
16:04:36,662 [Executor task launch worker for task 122] INFO  spark.executor.Executor         - Running task 0.0 in stage 61.0 (TID 122) 
16:04:36,662 [Executor task launch worker for task 123] INFO  spark.executor.Executor         - Running task 1.0 in stage 61.0 (TID 123) 
16:04:36,663 [Executor task launch worker for task 122] DEBUG spark.storage.BlockManager      - Getting local block broadcast_67 
16:04:36,663 [Executor task launch worker for task 122] DEBUG spark.storage.BlockManager      - Level for block broadcast_67 is StorageLevel(disk, memory, deserialized, 1 replicas) 
16:04:36,663 [Executor task launch worker for task 122] INFO  spark.executor.Executor         - Finished task 0.0 in stage 61.0 (TID 122). 614 bytes result sent to driver 
16:04:36,663 [Executor task launch worker for task 123] INFO  spark.executor.Executor         - Finished task 1.0 in stage 61.0 (TID 123). 614 bytes result sent to driver 
16:04:36,663 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_61.0, runningTasks: 1 
16:04:36,663 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSetManager  - No tasks for locality level NO_PREF, so moving to locality level ANY 
16:04:36,663 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_61.0, runningTasks: 0 
16:04:36,663 [task-result-getter-2] INFO  spark.scheduler.TaskSetManager  - Finished task 0.0 in stage 61.0 (TID 122) in 1 ms on localhost (executor driver) (1/2) 
16:04:36,663 [task-result-getter-3] INFO  spark.scheduler.TaskSetManager  - Finished task 1.0 in stage 61.0 (TID 123) in 1 ms on localhost (executor driver) (2/2) 
16:04:36,663 [task-result-getter-3] INFO  spark.scheduler.TaskSchedulerImpl  - Removed TaskSet 61.0, whose tasks have all completed, from pool  
16:04:36,664 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - ResultStage 61 (collect at JobClusterUtils.java:75) finished in 0.003 s 
16:04:36,664 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - After removal of stage 61, remaining stages = 0 
16:04:36,664 [main] INFO  spark.scheduler.DAGScheduler    - Job 58 finished: collect at JobClusterUtils.java:75, took 0.003552 s 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++ 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.$outer 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Object) 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collection.Iterator) 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 2 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 2 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      <function0> 
16:04:36,664 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++ 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply() 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer() 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 1 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 1 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      org.apache.spark.rdd.RDD 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 1 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      MapPartitionsRDD[24] at map at KMeansModel.scala:71 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 4 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class java.lang.Object,Set()) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class scala.runtime.AbstractFunction0,Set()) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer)) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1)) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[24] at map at KMeansModel.scala:71) 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++ 
16:04:36,665 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++ 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++ 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -  + declared fields: 2 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -  + declared methods: 2 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object) 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator) 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -  + inner classes: 0 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -  + outer classes: 0 
16:04:36,666 [main] DEBUG spark.util.ClosureCleaner       -  + outer objects: 0 
16:04:36,667 [main] DEBUG spark.util.ClosureCleaner       -  + populating accessed fields because this is the starting closure 
16:04:36,667 [main] DEBUG spark.util.ClosureCleaner       -  + fields accessed by starting closure: 0 
16:04:36,667 [main] DEBUG spark.util.ClosureCleaner       -  + there are no enclosing objects! 
16:04:36,667 [main] DEBUG spark.util.ClosureCleaner       -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++ 
16:04:36,667 [main] INFO  apache.spark.SparkContext       - Starting job: collect at JobClusterUtils.java:73 
16:04:36,667 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Got job 59 (collect at JobClusterUtils.java:73) with 2 output partitions 
16:04:36,667 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Final stage: ResultStage 62 (collect at JobClusterUtils.java:73) 
16:04:36,668 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Parents of final stage: List() 
16:04:36,668 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Missing parents: List() 
16:04:36,668 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitStage(ResultStage 62) 
16:04:36,668 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - missing: List() 
16:04:36,668 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting ResultStage 62 (MapPartitionsRDD[24] at map at KMeansModel.scala:71), which has no missing parents 
16:04:36,668 [dag-scheduler-event-loop] DEBUG spark.scheduler.DAGScheduler    - submitMissingTasks(ResultStage 62) 
16:04:36,668 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_68 stored as values in memory (estimated size 3.1 KB, free 1443.4 MB) 
16:04:36,669 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_68 locally took  1 ms 
16:04:36,669 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_68 without replication took  1 ms 
16:04:36,670 [dag-scheduler-event-loop] INFO  storage.memory.MemoryStore      - Block broadcast_68_piece0 stored as bytes in memory (estimated size 1904.0 B, free 1443.4 MB) 
16:04:36,670 [dispatcher-event-loop-0] INFO  spark.storage.BlockManagerInfo  - Added broadcast_68_piece0 in memory on DESKTOP-8F3633O:61346 (size: 1904.0 B, free: 1443.5 MB) 
16:04:36,670 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManagerMaster  - Updated info of block broadcast_68_piece0 
16:04:36,670 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Told master about block broadcast_68_piece0 
16:04:36,670 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Put block broadcast_68_piece0 locally took  0 ms 
16:04:36,670 [dag-scheduler-event-loop] DEBUG spark.storage.BlockManager      - Putting block broadcast_68_piece0 without replication took  0 ms 
16:04:36,670 [dag-scheduler-event-loop] INFO  apache.spark.SparkContext       - Created broadcast 68 from broadcast at DAGScheduler.scala:1039 
16:04:36,670 [dag-scheduler-event-loop] INFO  spark.scheduler.DAGScheduler    - Submitting 2 missing tasks from ResultStage 62 (MapPartitionsRDD[24] at map at KMeansModel.scala:71) (first 15 tasks are for partitions Vector(0, 1)) 
16:04:36,670 [dag-scheduler-event-loop] INFO  spark.scheduler.TaskSchedulerImpl  - Adding task set 62.0 with 2 tasks 
16:04:36,670 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Epoch for TaskSet 62.0: 3 
16:04:36,670 [dag-scheduler-event-loop] DEBUG spark.scheduler.TaskSetManager  - Valid locality levels for TaskSet 62.0: NO_PREF, ANY 
16:04:36,671 [dispatcher-event-loop-1] DEBUG spark.scheduler.TaskSchedulerImpl  - parentName: , name: TaskSet_62.0, runningTasks: 0 
16:04:36,671 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 0.0 in stage 62.0 (TID 124, localhost, executor driver, partition 0, PROCESS_LOCAL, 23638 bytes) 
16:04:36,671 [dispatcher-event-loop-1] INFO  spark.scheduler.TaskSetManager  - Starting task 1.0 in stage 62.0 (TID 125, localhost, executor driver, partition 1, PROCESS_LOCAL, 23638 bytes) 
